{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJUuppl3yXAi",
    "outputId": "048c064d-c506-4cf0-80b3-47bfb941fff8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pandas_ta"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVJ28m3FZWzq",
    "outputId": "5f43447e-b3a7-403d-900e-1b3e79211357"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import site\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, matthews_corrcoef,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "pandas_ta_path = None\n",
    "for sp in site.getsitepackages():\n",
    "    pandas_ta_path = os.path.join(sp, 'pandas_ta')\n",
    "    if os.path.exists(pandas_ta_path):\n",
    "        break\n",
    "\n",
    "if pandas_ta_path:\n",
    "    squeeze_pro_path = os.path.join(pandas_ta_path, 'momentum', 'squeeze_pro.py')\n",
    "    if os.path.exists(squeeze_pro_path):\n",
    "        try:\n",
    "            with open(squeeze_pro_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            new_lines = []\n",
    "            fixed = False\n",
    "            for line in lines:\n",
    "                if \"from numpy import NaN as npNaN\" in line:\n",
    "                    new_lines.append(line.replace(\"from numpy import NaN as npNaN\", \"# from numpy import NaN as npNaN\\nimport numpy as np\\n\"))\n",
    "                    fixed = True\n",
    "                    print(\"Modified import statement in squeeze_pro.py\")\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "\n",
    "            if fixed:\n",
    "                with open(squeeze_pro_path, 'w') as f:\n",
    "                    f.writelines(new_lines)\n",
    "                print(\"Successfully patched pandas_ta/momentum/squeeze_pro.py\")\n",
    "            else:\n",
    "                print(\"Could not find the problematic import line in squeeze_pro.py\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error modifying squeeze_pro.py: {e}\")\n",
    "    else:\n",
    "        print(f\"Could not find squeeze_pro.py at {squeeze_pro_path}\")\n",
    "else:\n",
    "    print(\"Could not find the pandas_ta library installation path.\")\n",
    "\n",
    "import pandas_ta as ta"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNaJg6uSZaXb",
    "outputId": "63f14caf-04bd-4e9d-ffa6-7e6bc8250109"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "5mgG7zbGDS_h"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Configuration Parameters ---\n",
    "MODEL_SAVE_PATH = \"trained_models/\"\n",
    "MIN_SEQUENCE_LENGTH = 12  # Minimum sequence length for any company\n",
    "MAX_SEQUENCE_LENGTH = 12  # Maximum sequence length to cap computational cost\n",
    "INITIAL_TRAINING_DAYS = 1100  # Number of days to use for initial training only\n",
    "KELLY_FRACTION = 0.05\n",
    "SECTOR_CONFIDENCE_THRESHOLD = 0.40\n",
    "RETRAIN_INTERVAL = 200\n",
    "MAX_DAY_GAP = 5  # Maximum allowed gap in trading days (to account for weekends/holidays)"
   ],
   "metadata": {
    "id": "j4GsF7NZDVbn"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def identify_contiguous_periods(df: pd.DataFrame, max_gap_days: int = MAX_DAY_GAP) -> list:\n",
    "\n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    dates = pd.to_datetime(df['date'])\n",
    "\n",
    "    contiguous_periods = []\n",
    "    start_idx = 0\n",
    "\n",
    "    for i in range(1, len(dates)):\n",
    "        gap = (dates[i] - dates[i-1]).days\n",
    "        if gap > max_gap_days:\n",
    "            # End current period and start new one\n",
    "            contiguous_periods.append((start_idx, i-1))\n",
    "            start_idx = i\n",
    "\n",
    "\n",
    "    contiguous_periods.append((start_idx, len(dates)-1))\n",
    "\n",
    "\n",
    "    contiguous_periods = [(s, e) for s, e in contiguous_periods if e - s >= MIN_SEQUENCE_LENGTH]\n",
    "\n",
    "    return contiguous_periods"
   ],
   "metadata": {
    "id": "PuYsKuvdDVZU"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_dynamic_sequence_length(company_data_df: pd.DataFrame,\n",
    "                                     min_length: int = MIN_SEQUENCE_LENGTH,\n",
    "                                     max_length: int = MAX_SEQUENCE_LENGTH,\n",
    "                                     target_fraction: float = 0.15) -> int:\n",
    "\n",
    "    total_days = len(company_data_df)\n",
    "\n",
    "\n",
    "    dynamic_length = int(total_days * target_fraction)\n",
    "    dynamic_length = max(min_length, min(dynamic_length, max_length))\n",
    "\n",
    "    return dynamic_length"
   ],
   "metadata": {
    "id": "fPwHBx-cDVXP"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def create_contiguous_sequences(data: np.ndarray, targets: np.ndarray,\n",
    "                               contiguous_periods: list, sequence_length: int):\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for start_idx, end_idx in contiguous_periods:\n",
    "        period_length = end_idx - start_idx + 1\n",
    "        if period_length < sequence_length:\n",
    "            continue\n",
    "\n",
    "        for i in range(start_idx + sequence_length, end_idx + 1):\n",
    "            X.append(data[i-sequence_length:i])\n",
    "            y.append(targets[i])\n",
    "\n",
    "    return np.array(X) if X else np.array([]), np.array(y) if y else np.array([])"
   ],
   "metadata": {
    "id": "UuNqQe0sDVVI"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def create_target_variable(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    print(\"Creating target variable...\")\n",
    "    df = df.sort_values(by=['ticker', 'date']).copy()\n",
    "    df['next_day_close'] = df.groupby('ticker')['close_price'].shift(-1)\n",
    "    df['target'] = (df['next_day_close'] > df['close_price']).astype(int)\n",
    "    df.dropna(subset=['next_day_close'], inplace=True)\n",
    "    df['target'] = df['target'].astype(int)\n",
    "    print(\"Target variable created.\")\n",
    "    return df"
   ],
   "metadata": {
    "id": "AofHGE27DVS-"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_historical_payouts(df: pd.DataFrame) -> dict:\n",
    "\n",
    "    print(\"Calculating historical payouts for Kelly Criterion...\")\n",
    "    winning_days = df[df['target'] == 1].copy()\n",
    "    winning_days['payout'] = (winning_days['next_day_close'] - winning_days['close_price']) / winning_days['close_price']\n",
    "    payout_map = winning_days.groupby('ticker')['payout'].mean().to_dict()\n",
    "    print(\"Payouts calculated.\")\n",
    "    return payout_map"
   ],
   "metadata": {
    "id": "DDQCys6vDVQr"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def train_company_models(company_data_df: pd.DataFrame, ticker: str,\n",
    "                        feature_cols: list, model_save_path: str,\n",
    "                        sequence_length: int = None):\n",
    "\n",
    "    if sequence_length is None:\n",
    "        sequence_length = calculate_dynamic_sequence_length(company_data_df)\n",
    "\n",
    "    if len(company_data_df) < sequence_length + 10:\n",
    "        print(f\"Not enough data for {ticker}.\")\n",
    "        return False, sequence_length\n",
    "\n",
    "\n",
    "    contiguous_periods = identify_contiguous_periods(company_data_df)\n",
    "    if not contiguous_periods:\n",
    "        print(f\"No contiguous periods found for {ticker}.\")\n",
    "        return False, sequence_length\n",
    "\n",
    "\n",
    "    targets = company_data_df['target'].values\n",
    "\n",
    "    X, y = create_contiguous_sequences(company_data_df[feature_cols].values,\n",
    "                                       targets,\n",
    "                                       contiguous_periods,\n",
    "                                       sequence_length)\n",
    "\n",
    "    print(f\"Created {len(X)} sequences for {ticker}.\")\n",
    "\n",
    "    if len(X) < 2:\n",
    "        return False, sequence_length\n",
    "\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "\n",
    "\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', 'precision', 'recall']\n",
    "            )\n",
    "\n",
    "    callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7, verbose=0)\n",
    "                ]\n",
    "\n",
    "    history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    validation_predictions = model.predict(X_val_scaled, verbose=0).flatten()\n",
    "\n",
    "    buffer = 0.1 * (validation_predictions.max() - validation_predictions.min())  # 5% of range\n",
    "    y_min_dynamic = validation_predictions.min() - buffer\n",
    "    y_max_dynamic = validation_predictions.max() + buffer\n",
    "\n",
    "\n",
    "    y_min_dynamic = max(0.0, y_min_dynamic)\n",
    "    y_max_dynamic = min(1.0, y_max_dynamic)\n",
    "\n",
    "    calibrator = IsotonicRegression(y_min=y_min_dynamic, y_max=y_max_dynamic, out_of_bounds='clip')\n",
    "    calibrator.fit(validation_predictions, y_val)\n",
    "\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    model.save(os.path.join(model_save_path, f\"{ticker}_lstm.keras\"))\n",
    "    joblib.dump(calibrator, os.path.join(model_save_path, f\"{ticker}_calibrator.pkl\"))\n",
    "    joblib.dump(scaler, os.path.join(model_save_path, f\"{ticker}_scaler.pkl\"))\n",
    "    joblib.dump(sequence_length, os.path.join(model_save_path, f\"{ticker}_seq_length.pkl\"))\n",
    "\n",
    "    del model, scaler, calibrator, X_train_scaled, X_val_scaled\n",
    "    tf.keras.backend.clear_session()\n",
    "    del X_train, X_val, y_train, y_val\n",
    "    gc.collect()\n",
    "\n",
    "    return True, sequence_length"
   ],
   "metadata": {
    "id": "Fx57Rq9KDVOe"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_next_day_performance(company_data_df: pd.DataFrame, ticker: str,\n",
    "                                feature_cols: list, model_save_path: str) -> dict:\n",
    "\n",
    "    try:\n",
    "\n",
    "        model = keras.models.load_model(os.path.join(model_save_path, f\"{ticker}_lstm.keras\"))\n",
    "        calibrator = joblib.load(os.path.join(model_save_path, f\"{ticker}_calibrator.pkl\"))\n",
    "        scaler = joblib.load(os.path.join(model_save_path, f\"{ticker}_scaler.pkl\"))\n",
    "        sequence_length = joblib.load(os.path.join(model_save_path, f\"{ticker}_seq_length.pkl\"))\n",
    "    except IOError:\n",
    "        return None\n",
    "\n",
    "    contiguous_periods = identify_contiguous_periods(company_data_df)\n",
    "    if not contiguous_periods:\n",
    "        return None\n",
    "\n",
    "    last_period_start, last_period_end = contiguous_periods[-1]\n",
    "    period_data = company_data_df.iloc[last_period_start:last_period_end+1]\n",
    "\n",
    "    if len(period_data) < sequence_length:\n",
    "        return None\n",
    "\n",
    "    last_sequence = period_data.tail(sequence_length)\n",
    "    scaled_features = scaler.transform(last_sequence[feature_cols])\n",
    "    input_sequence = np.array([scaled_features])\n",
    "\n",
    "    raw_prediction = model.predict(input_sequence, verbose=0)[0][0]\n",
    "    calibrated_prediction = calibrator.predict([raw_prediction])[0]\n",
    "\n",
    "    print(f\"Raw prediction for {ticker}: {raw_prediction}\")\n",
    "    print(f\"Calibrated prediction for {ticker}: {calibrated_prediction}\")\n",
    "\n",
    "    return {\n",
    "        'ticker': ticker,\n",
    "        'raw_prediction': raw_prediction,\n",
    "        'calibrated_prediction': calibrated_prediction\n",
    "    }"
   ],
   "metadata": {
    "id": "srByLxojDVIw"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def select_and_size_portfolio(daily_predictions_df: pd.DataFrame, payout_map: dict,\n",
    "                            total_capital: float, sector_threshold: float,\n",
    "                            kelly_fraction: float) -> pd.DataFrame:\n",
    "\n",
    "    investment_decisions = []\n",
    "\n",
    "    if daily_predictions_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for sector, group in daily_predictions_df.groupby('sector'):\n",
    "        avg_sector_score = group['calibrated_prediction'].mean()\n",
    "        if avg_sector_score < sector_threshold:\n",
    "            continue\n",
    "\n",
    "        best_stock_in_sector = group.loc[group['calibrated_prediction'].idxmax()]\n",
    "        ticker = best_stock_in_sector['ticker']\n",
    "        p = best_stock_in_sector['calibrated_prediction']\n",
    "        b = payout_map.get(ticker, 0)\n",
    "\n",
    "        if b <= 0:\n",
    "            continue\n",
    "\n",
    "        kelly_percentage = p - ((1 - p) / b)\n",
    "\n",
    "        if kelly_percentage > 0:\n",
    "            investment_fraction = kelly_percentage * kelly_fraction\n",
    "            investment_amount = total_capital * investment_fraction\n",
    "\n",
    "            print(f\"Investing {investment_amount} in {ticker}\")\n",
    "\n",
    "            investment_decisions.append({\n",
    "                'ticker': ticker,\n",
    "                'investment_fraction': investment_fraction,\n",
    "                'investment_amount': investment_amount,\n",
    "                'predicted_prob': p\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(investment_decisions)"
   ],
   "metadata": {
    "id": "EJrxMx9uEoBp"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def models_exist_for_ticker(ticker, model_path):\n",
    "\n",
    "    lstm_path = os.path.join(model_path, f\"{ticker}_lstm.keras\")\n",
    "    calibrator_path = os.path.join(model_path, f\"{ticker}_calibrator.pkl\")\n",
    "    scaler_path = os.path.join(model_path, f\"{ticker}_scaler.pkl\")\n",
    "    seq_length_path = os.path.join(model_path, f\"{ticker}_seq_length.pkl\")\n",
    "    return (os.path.exists(lstm_path) and os.path.exists(calibrator_path) and\n",
    "            os.path.exists(scaler_path) and os.path.exists(seq_length_path))"
   ],
   "metadata": {
    "id": "NevAdFjdEn9G"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def run_simulation(master_df: pd.DataFrame, payout_map: dict, feature_cols: list,\n",
    "                  initial_capital: float, initial_training_days: int = INITIAL_TRAINING_DAYS):\n",
    "\n",
    "    print(f\"\\nStarting simulation with {initial_training_days} initial training days...\")\n",
    "    capital = initial_capital\n",
    "    simulation_log = []\n",
    "\n",
    "    all_tickers = master_df['ticker'].unique()\n",
    "    retrain_counter = {ticker: RETRAIN_INTERVAL for ticker in all_tickers}\n",
    "    ticker_sequence_lengths = {}\n",
    "\n",
    "    unique_dates = sorted(master_df['date'].unique())\n",
    "\n",
    "\n",
    "    start_index = min(initial_training_days, len(unique_dates) - 1)\n",
    "\n",
    "    print(f\"Starting predictions from day {start_index} (after {initial_training_days} training days)\")\n",
    "\n",
    "    for i in tqdm(range(start_index, len(unique_dates)), desc=\"Simulating Trading Days\"):\n",
    "        current_date = unique_dates[i]\n",
    "\n",
    "        historical_data = master_df[master_df['date'] < current_date]\n",
    "        todays_data_for_prediction = master_df[master_df['date'] == unique_dates[i-1]]\n",
    "        next_day_data = master_df[master_df['date'] == current_date]\n",
    "\n",
    "        daily_predictions = []\n",
    "\n",
    "        for ticker in todays_data_for_prediction['ticker'].unique():\n",
    "            company_hist_data = historical_data[historical_data['ticker'] == ticker]\n",
    "            if company_hist_data.empty:\n",
    "                continue\n",
    "\n",
    "\n",
    "            if retrain_counter.get(ticker, 0) >= RETRAIN_INTERVAL or not models_exist_for_ticker(ticker, MODEL_SAVE_PATH):\n",
    "                training_success, seq_length = train_company_models(\n",
    "                    company_hist_data, ticker, feature_cols, MODEL_SAVE_PATH\n",
    "                )\n",
    "                if training_success:\n",
    "                    ticker_sequence_lengths[ticker] = seq_length\n",
    "                    retrain_counter[ticker] = 0\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            prediction_result = predict_next_day_performance(\n",
    "                company_hist_data, ticker, feature_cols, MODEL_SAVE_PATH\n",
    "            )\n",
    "\n",
    "            if prediction_result:\n",
    "                info = todays_data_for_prediction[todays_data_for_prediction['ticker'] == ticker].iloc[0]\n",
    "                prediction_result.update({'company_name': info['company_name'], 'sector': info['sector']})\n",
    "                daily_predictions.append(prediction_result)\n",
    "                retrain_counter[ticker] += 1\n",
    "\n",
    "        daily_predictions_df = pd.DataFrame(daily_predictions)\n",
    "        investment_decision_df = select_and_size_portfolio(\n",
    "            daily_predictions_df, payout_map, capital, SECTOR_CONFIDENCE_THRESHOLD, KELLY_FRACTION\n",
    "        )\n",
    "\n",
    "        capital_at_start_of_day = capital\n",
    "        total_pnl = 0\n",
    "        invested_capital = 0\n",
    "\n",
    "        if not investment_decision_df.empty:\n",
    "            invested_capital = investment_decision_df['investment_amount'].sum()\n",
    "            capital -= invested_capital\n",
    "\n",
    "            for _, trade in investment_decision_df.iterrows():\n",
    "                ticker = trade['ticker']\n",
    "                investment_amount = trade['investment_amount']\n",
    "                outcome = next_day_data[next_day_data['ticker'] == ticker]\n",
    "                if not outcome.empty:\n",
    "                    prev_close = todays_data_for_prediction[todays_data_for_prediction['ticker'] == ticker].iloc[0]['close_price']\n",
    "                    actual_return = outcome.iloc[0]['close_price'] / prev_close - 1\n",
    "                    pnl = investment_amount * actual_return\n",
    "                    total_pnl += pnl\n",
    "                    capital += (investment_amount + pnl)\n",
    "\n",
    "        simulation_log.append({\n",
    "            'date': current_date,\n",
    "            'capital_start': capital_at_start_of_day,\n",
    "            'capital_end': capital,\n",
    "            'daily_pnl': total_pnl,\n",
    "            'investments_made': investment_decision_df.to_dict('records')\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(simulation_log)"
   ],
   "metadata": {
    "id": "iJKEMPL_EuTk"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_final_results(simulation_log: pd.DataFrame, initial_capital: float):\n",
    "\n",
    "    if simulation_log.empty:\n",
    "        print(\"Simulation log is empty. No results to calculate.\")\n",
    "        return\n",
    "\n",
    "    final_capital = simulation_log['capital_end'].iloc[-1]\n",
    "    total_roi = (final_capital - initial_capital) / initial_capital\n",
    "    simulation_log['daily_return'] = (simulation_log['capital_end'] / simulation_log['capital_start']) - 1\n",
    "\n",
    "    if simulation_log['daily_return'].std() > 0:\n",
    "        sharpe_ratio = (simulation_log['daily_return'].mean() / simulation_log['daily_return'].std()) * np.sqrt(252)\n",
    "    else:\n",
    "        sharpe_ratio = 0.0\n",
    "\n",
    "    print(\"\\n--- Simulation Results ---\")\n",
    "    print(f\"Initial Capital: ${initial_capital:,.2f}\")\n",
    "    print(f\"Final Capital:   ${final_capital:,.2f}\")\n",
    "    print(f\"Total Return on Investment (ROI): {total_roi:.2%}\")\n",
    "    print(f\"Annualized Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "    print(\"--------------------------\")"
   ],
   "metadata": {
    "id": "EPZYSTZVEuQ8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "companies = pd.read_parquet('/content/stock_table.parquet')\n",
    "tweets = pd.read_parquet('/content/stock_tweets_withsentiment_withemotion_withstance_nomerge.parquet')\n",
    "stocks = pd.read_parquet('/content/stock_prices.parquet')\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "companies.columns = [x.lower() for x in companies.columns]\n",
    "tweets.columns = [x.lower() for x in tweets.columns]\n",
    "stocks.columns = [x.lower() for x in stocks.columns]\n",
    "\n",
    "tweets['stance_positive'] = (tweets['stance_label'] == 'Positive').astype(int)\n",
    "tweets['stance_negative'] = (tweets['stance_label'] == 'Negative').astype(int)\n",
    "\n",
    "tweets_merged = tweets.groupby(['date', 'ticker'], as_index=False).agg({\n",
    "    'text': lambda x: ' '.join(x),\n",
    "    'sentiment': lambda x: x.mean(),\n",
    "    'emotion_anger': 'sum',\n",
    "    'emotion_disgust': 'sum',\n",
    "    'emotion_fear': 'sum',\n",
    "    'emotion_joy': 'sum',\n",
    "    'emotion_neutral': 'sum',\n",
    "    'emotion_sadness': 'sum',\n",
    "    'emotion_surprize': 'sum',\n",
    "    'stance_positive': 'sum',\n",
    "    'stance_negative': 'sum'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tweets_merged['date'] = pd.to_datetime(tweets_merged['date'])\n",
    "stocks['date'] = pd.to_datetime(stocks['date'])\n",
    "\n",
    "\"\"\"\n",
    "master_df = stocks.merge(\n",
    "    tweets_merged,\n",
    "    on=[\"date\", \"ticker\"]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "master_df = pd.merge(\n",
    "    stocks,\n",
    "    tweets_merged,\n",
    "    on=[\"date\", \"ticker\"],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "tweet_feature_cols = ['sentiment', 'emotion_anger', 'emotion_disgust', 'emotion_fear', 'emotion_joy', 'emotion_neutral', 'emotion_sadness', 'emotion_surprize', 'stance_positive', 'stance_negative']\n",
    "for col in tweet_feature_cols:\n",
    "    if col in master_df.columns:\n",
    "        master_df[col].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "master_df = pd.merge(master_df, companies[['ticker', 'sector', 'company']], on='ticker', how='left')\n",
    "\n",
    "\n",
    "feature_cols = ['open','high','low','volume']\n",
    "\n",
    "master_df = master_df.rename(columns={'close': 'close_price', 'company': 'company_name'})\n",
    "\n",
    "\n",
    "print(f\"Shape of master_df before dropping NaNs: {master_df.shape}\")\n",
    "#master_df.dropna(inplace=True)\n",
    "print(f\"Shape of master_df after dropping NaNs: {master_df.shape}\")\n",
    "\n",
    "master_df.rename(columns={'close_price': 'close'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "master_df.sort_values(by=['ticker', 'date'], inplace=True)\n",
    "\n",
    "\n",
    "def apply_ta_indicators(df_group):\n",
    "    df_group.set_index(pd.DatetimeIndex(df_group['date']), inplace=True)\n",
    "    #Trend\n",
    "    df_group.ta.ema(length=12, append=True)\n",
    "    df_group.ta.ema(length=26, append=True)\n",
    "    df_group.ta.ema(length=50, append=True)\n",
    "\n",
    "    df_group.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "\n",
    "\n",
    "\n",
    "    df_group.ta.rsi(length=14, append=True)\n",
    "    df_group.ta.stochrsi(length=14, append=True)\n",
    "\n",
    "\n",
    "    df_group.ta.atr(length=14, append=True)\n",
    "\n",
    "    bb = ta.bbands(df_group['close'], length=20, std=2)\n",
    "    df_group['BB_upper'] = bb['BBU_20_2.0']\n",
    "    df_group['BB_middle'] = bb['BBM_20_2.0']\n",
    "    df_group['BB_lower'] = bb['BBL_20_2.0']\n",
    "\n",
    "\n",
    "    df_group.ta.obv(append=True)\n",
    "    return df_group.reset_index(drop=True)\n",
    "\n",
    "master_df = master_df.groupby('ticker').apply(apply_ta_indicators)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ro6vATRZDsPr",
    "outputId": "0c1f88d2-7318-42c6-d0a1-f0e7b89634c4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "master_df.drop(columns=['text','adj close','sentiment','emotion_anger','emotion_disgust','emotion_fear','emotion_joy','emotion_neutral','emotion_sadness','emotion_surprize'], inplace=True)\n"
   ],
   "metadata": {
    "id": "6t_R9-flat8q"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "master_df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "8JLm7c3TavpM",
    "outputId": "4b788c56-26d4-4568-ca86-fb38e43b903c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "columns_to_check = ['EMA_12', 'EMA_26','EMA_50','MACD_12_26_9','MACDh_12_26_9','MACDs_12_26_9','RSI_14','ATRr_14','STOCHRSIk_14_14_3_3','STOCHRSId_14_14_3_3','ATRr_14','BB_upper','BB_middle','BB_lower','OBV']\n",
    "master_df = master_df.dropna(subset=columns_to_check)"
   ],
   "metadata": {
    "id": "37ocQuphavmv"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "feature_cols = ['open','high','low','volume'\n",
    "                ,'stance_positive','stance_negative'\n",
    "                ]\n",
    "\n",
    "new_indicator_columns = [\n",
    "    'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9',\n",
    "    'RSI_14', 'ATRr_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3',\n",
    "    'BB_upper', 'BB_middle', 'BB_lower', 'OBV'\n",
    "]\n",
    "feature_cols.extend(new_indicator_columns)"
   ],
   "metadata": {
    "id": "Km4RAlZAavkp"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "master_df = master_df.rename(columns={'close': 'close_price', 'company': 'company_name'})\n"
   ],
   "metadata": {
    "id": "M17bRZDeaviL"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "master_df.reset_index(drop=True, inplace=True)\n",
    "master_df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "qXxxJfwKavWp",
    "outputId": "f7722923-966b-4bb7-a52b-fc850b86788d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "master_df_with_target = create_target_variable(master_df)\n",
    "payout_map = calculate_historical_payouts(master_df_with_target)\n",
    "\n",
    "initial_capital = 100_000.0\n",
    "simulation_results = run_simulation(\n",
    "    master_df_with_target,\n",
    "    payout_map,\n",
    "    feature_cols,\n",
    "    initial_capital,\n",
    "    initial_training_days=INITIAL_TRAINING_DAYS\n",
    ")\n",
    "\n",
    "display(simulation_results.head(100))\n",
    "calculate_final_results(simulation_results, initial_capital)"
   ],
   "metadata": {
    "id": "OvyY1VqMDCL-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3a4d06d2-1875-40d7-911d-a6ff62edb7f6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display(simulation_results.head(100))"
   ],
   "metadata": {
    "id": "Z8FLED4uUV6r"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "folder_path = '/content/trained_models/'\n",
    "if os.path.exists(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "else:\n",
    "    print(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "print(f\"Contents of {folder_path} after deletion attempt:\")\n",
    "if os.path.exists(folder_path):\n",
    "    print(os.listdir(folder_path))\n",
    "else:\n",
    "    print(\"Folder does not exist.\")"
   ],
   "metadata": {
    "id": "-Wb2ljWIXNsF"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
