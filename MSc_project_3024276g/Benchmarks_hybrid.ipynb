{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install pandas_ta\n",
    "!pip install --upgrade pandas_ta"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgZ_ylAADNb6",
    "outputId": "6134cc02-4bf2-4dbb-a201-b8fd4d0df03c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "befeuglqHy8f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a2bcece3-930c-4c77-c6ff-3473d1fde791"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import site\n",
    "import os\n",
    "\n",
    "# Find the path to the pandas_ta library and patch it\n",
    "pandas_ta_path = None\n",
    "for sp in site.getsitepackages():\n",
    "    pandas_ta_path = os.path.join(sp, 'pandas_ta')\n",
    "    if os.path.exists(pandas_ta_path):\n",
    "        break\n",
    "\n",
    "if pandas_ta_path:\n",
    "    squeeze_pro_path = os.path.join(pandas_ta_path, 'momentum', 'squeeze_pro.py')\n",
    "    if os.path.exists(squeeze_pro_path):\n",
    "        try:\n",
    "            with open(squeeze_pro_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            new_lines = []\n",
    "            fixed = False\n",
    "            for line in lines:\n",
    "                if \"from numpy import NaN as npNaN\" in line:\n",
    "                    new_lines.append(line.replace(\"from numpy import NaN as npNaN\", \"# from numpy import NaN as npNaN\\nimport numpy as np\\n\"))\n",
    "                    fixed = True\n",
    "                    print(\"Modified import statement in squeeze_pro.py\")\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "\n",
    "            if fixed:\n",
    "                with open(squeeze_pro_path, 'w') as f:\n",
    "                    f.writelines(new_lines)\n",
    "                print(\"Successfully patched pandas_ta/momentum/squeeze_pro.py\")\n",
    "            else:\n",
    "                print(\"Could not find the problematic import line in squeeze_pro.py\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error modifying squeeze_pro.py: {e}\")\n",
    "    else:\n",
    "        print(f\"Could not find squeeze_pro.py at {squeeze_pro_path}\")\n",
    "else:\n",
    "    print(\"Could not find the pandas_ta library installation path.\")\n",
    "\n",
    "# Now import pandas_ta after patching\n",
    "import pandas_ta as ta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, matthews_corrcoef,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class StockPredictionPipeline:\n",
    "    def __init__(self, df, feature_columns, model_type='LSTM', sequence_length=30, problem_type='regression'):\n",
    "\n",
    "        self.df = df.copy()\n",
    "        self.feature_columns = feature_columns\n",
    "        self.model_type = model_type\n",
    "        self.sequence_length = sequence_length\n",
    "        self.problem_type = problem_type\n",
    "        self.results = []\n",
    "\n",
    "        # Validate inputs\n",
    "        self._validate_inputs()\n",
    "\n",
    "        print(f\"Pipeline initialized for a '{self.problem_type}' problem.\")\n",
    "\n",
    "    def _validate_inputs(self):\n",
    "        \"\"\"Validate input parameters and data.\"\"\"\n",
    "        # Check if required columns exist\n",
    "        missing_cols = [col for col in self.feature_columns if col not in self.df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing feature columns: {missing_cols}\")\n",
    "\n",
    "        # Check for price column\n",
    "        if 'close' not in self.df.columns and 'close_price' not in self.df.columns:\n",
    "            raise ValueError(\"No 'close' or 'close_price' column found in data\")\n",
    "\n",
    "        # Check model type\n",
    "        valid_models = ['LSTM', 'BiLSTM', 'GRU', 'BiGRU']\n",
    "        if self.model_type not in valid_models:\n",
    "            raise ValueError(f\"Model type must be one of: {valid_models}\")\n",
    "\n",
    "        # Check problem type\n",
    "        if self.problem_type not in ['regression', 'classification']:\n",
    "            raise ValueError(\"Problem type must be 'regression' or 'classification'\")\n",
    "\n",
    "    def create_target_variable(self, company_data):\n",
    "\n",
    "        # Make a copy to avoid SettingWithCopyWarning\n",
    "        company_data = company_data.copy()\n",
    "\n",
    "        price_col = 'close' if 'close' in company_data.columns else 'close_price'\n",
    "\n",
    "        # Ensure data is sorted by date if date column exists\n",
    "        if 'date' in company_data.columns:\n",
    "            company_data = company_data.sort_values('date')\n",
    "\n",
    "        # Calculate log returns for better statistical properties\n",
    "        company_data['target_regression'] = (\n",
    "            np.log(company_data[price_col].shift(-1)) - np.log(company_data[price_col])\n",
    "        )\n",
    "\n",
    "        # Create binary direction target\n",
    "        company_data['target_direction'] = (company_data['target_regression'] > 0).astype(int)\n",
    "\n",
    "        # Remove rows with NaN values\n",
    "        company_data = company_data.dropna()\n",
    "\n",
    "        return company_data\n",
    "\n",
    "    def create_sequences(self, features, *targets):\n",
    "\n",
    "        X = []\n",
    "        y_sequences = [[] for _ in targets]\n",
    "\n",
    "        for i in range(self.sequence_length, len(features)):\n",
    "            X.append(features[i-self.sequence_length:i])\n",
    "            for j, target in enumerate(targets):\n",
    "                y_sequences[j].append(target[i])\n",
    "\n",
    "        return (np.array(X),) + tuple(np.array(y) for y in y_sequences)\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "\n",
    "        # Set random seeds for reproducibility\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        # Build RNN layers based on model type\n",
    "        if self.model_type == 'LSTM':\n",
    "            model.add(layers.LSTM(128, return_sequences=True, input_shape=input_shape, dropout=0.1, recurrent_dropout=0.1))\n",
    "            model.add(layers.LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "        elif self.model_type == 'BiLSTM':\n",
    "            model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1), input_shape=input_shape))\n",
    "            model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1)))\n",
    "        elif self.model_type == 'GRU':\n",
    "            model.add(layers.GRU(128, return_sequences=True, input_shape=input_shape, dropout=0.1, recurrent_dropout=0.1))\n",
    "            model.add(layers.GRU(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "        elif self.model_type == 'BiGRU':\n",
    "            model.add(layers.Bidirectional(layers.GRU(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1), input_shape=input_shape))\n",
    "            model.add(layers.Bidirectional(layers.GRU(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1)))\n",
    "\n",
    "        # Add batch normalization and dense layers\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dense(32, activation='relu'))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        # Problem-specific output layer\n",
    "        if self.problem_type == 'regression':\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            model.compile(\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "                loss='huber',  # More robust than MSE for outliers\n",
    "                metrics=['mae', 'mse']\n",
    "            )\n",
    "        else:  # classification\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "            model.compile(\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', 'precision', 'recall']\n",
    "            )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def process_company(self, company_name, company_data, sector):\n",
    "\n",
    "        print(f\"\\nProcessing {company_name} ({sector})...\")\n",
    "\n",
    "        try:\n",
    "            company_data = self.create_target_variable(company_data)\n",
    "\n",
    "            # Check for sufficient data\n",
    "            min_samples = self.sequence_length + 150  # Increased minimum\n",
    "            if len(company_data) < min_samples:\n",
    "                print(f\"Insufficient data for {company_name} ({len(company_data)} < {min_samples}). Skipping...\")\n",
    "                return None\n",
    "\n",
    "            # Check for data quality issues\n",
    "            if company_data[self.feature_columns].isnull().any().any():\n",
    "                print(f\"Missing values in features for {company_name}. Skipping...\")\n",
    "                return None\n",
    "\n",
    "            features = company_data[self.feature_columns].values\n",
    "            target_reg = company_data['target_regression'].values\n",
    "            target_dir = company_data['target_direction'].values\n",
    "\n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "            # Create sequences\n",
    "            X, y_reg, y_dir = self.create_sequences(features_scaled, target_reg, target_dir)\n",
    "\n",
    "            # Use time series split to avoid data leakage\n",
    "            n_splits = min(5, len(X) // 50)  # Adaptive number of splits\n",
    "            if n_splits < 3:\n",
    "                print(f\"Insufficient data for proper time series validation for {company_name}. Skipping...\")\n",
    "                return None\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "            splits = list(tscv.split(X))\n",
    "            train_idx, test_idx = splits[-1]  # Use the last split for final training\n",
    "\n",
    "            # Further split train into train/validation\n",
    "            val_size = int(0.2 * len(train_idx))\n",
    "            final_train_idx = train_idx[:-val_size]\n",
    "            val_idx = train_idx[-val_size:]\n",
    "\n",
    "            X_train, X_val, X_test = X[final_train_idx], X[val_idx], X[test_idx]\n",
    "\n",
    "            # Choose target based on problem type\n",
    "            if self.problem_type == 'regression':\n",
    "                y_train, y_val, y_test = y_reg[final_train_idx], y_reg[val_idx], y_reg[test_idx]\n",
    "\n",
    "                # Scale targets for regression\n",
    "                target_scaler = StandardScaler()\n",
    "                y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "                y_val_scaled = target_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "\n",
    "                train_target = y_train_scaled\n",
    "                val_target = y_val_scaled\n",
    "\n",
    "            else:  # classification\n",
    "                y_train, y_val, y_test = y_dir[final_train_idx], y_dir[val_idx], y_dir[test_idx]\n",
    "                train_target = y_train\n",
    "                val_target = y_val\n",
    "                target_scaler = None\n",
    "\n",
    "            # Check class balance for classification\n",
    "            if self.problem_type == 'classification':\n",
    "                class_ratio = np.mean(y_train)\n",
    "                if class_ratio < 0.1 or class_ratio > 0.9:\n",
    "                    print(f\"Severe class imbalance for {company_name} ({class_ratio:.3f}). Consider using class weights.\")\n",
    "\n",
    "            # Build and train model\n",
    "            model = self.build_model((self.sequence_length, len(self.feature_columns)))\n",
    "\n",
    "            # Enhanced callbacks\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7, verbose=0)\n",
    "            ]\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train, train_target,\n",
    "                validation_data=(X_val, val_target),\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            y_pred = model.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                if target_scaler is not None:\n",
    "                    y_pred_unscaled = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "                else:\n",
    "                    y_pred_unscaled = y_pred\n",
    "\n",
    "                # Regression metrics\n",
    "                mse = mean_squared_error(y_test, y_pred_unscaled)\n",
    "                mae = mean_absolute_error(y_test, y_pred_unscaled)\n",
    "                r2 = r2_score(y_test, y_pred_unscaled)\n",
    "\n",
    "                # Derive directional accuracy from regression predictions\n",
    "                y_test_dir = (y_reg[test_idx] > 0).astype(int)\n",
    "                y_pred_dir = (y_pred_unscaled > 0).astype(int)\n",
    "\n",
    "            else:  # classification\n",
    "                # Classification metrics\n",
    "                y_pred_dir = (y_pred > 0.5).astype(int)\n",
    "                y_test_dir = y_test\n",
    "\n",
    "                # Set dummy regression metrics\n",
    "                mse = mae = r2 = np.nan\n",
    "\n",
    "            precision = precision_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "            recall = recall_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "            f1 = f1_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "            mcc = matthews_corrcoef(y_test_dir, y_pred_dir)\n",
    "\n",
    "            directional_accuracy = np.mean(y_test_dir == y_pred_dir)\n",
    "\n",
    "            result = {\n",
    "                'company': company_name,\n",
    "                'sector': sector,\n",
    "                'model_type': self.model_type,\n",
    "                'problem_type': self.problem_type,\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'r2': r2,\n",
    "                'mcc': mcc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'directional_accuracy': directional_accuracy,\n",
    "\n",
    "                'n_samples': len(X),\n",
    "                'train_samples': len(X_train),\n",
    "                'val_samples': len(X_val),\n",
    "                'test_samples': len(X_test),\n",
    "                'epochs_trained': len(history.history['loss'])\n",
    "            }\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                print(f\"  Regression -> MSE: {mse:.6f}, MAE: {mae:.6f}, R²: {r2:.4f}\")\n",
    "            print(f\"  Directional -> Accuracy: {directional_accuracy:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "            keras.backend.clear_session()\n",
    "            del model\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {company_name}: {str(e)}\")\n",
    "            keras.backend.clear_session()  # Clean up even on error\n",
    "            return None\n",
    "\n",
    "    def run_pipeline(self):\n",
    "\n",
    "        company_col = None\n",
    "        for col_name in ['ticker', 'company', 'symbol']:\n",
    "            if col_name in self.df.columns:\n",
    "                company_col = col_name\n",
    "                break\n",
    "\n",
    "        if company_col is None:\n",
    "            company_col = self.df.columns[0]\n",
    "            print(f\"Warning: Using '{company_col}' as company identifier column\")\n",
    "\n",
    "        companies = self.df[company_col].unique()\n",
    "        print(f\"Processing {len(companies)} companies with {self.model_type} model...\")\n",
    "        print(f\"Problem type: {self.problem_type}\")\n",
    "        print(f\"Sequence length: {self.sequence_length}\")\n",
    "        print(f\"Features: {self.feature_columns}\")\n",
    "\n",
    "        successful_companies = 0\n",
    "        for i, company in enumerate(companies, 1):\n",
    "            print(f\"\\n[{i}/{len(companies)}] Processing {company}...\")\n",
    "\n",
    "            company_data = self.df[self.df[company_col] == company].copy()\n",
    "            sector = company_data['sector'].iloc[0] if 'sector' in company_data.columns else 'Unknown'\n",
    "\n",
    "            result = self.process_company(company, company_data, sector)\n",
    "            if result:\n",
    "                self.results.append(result)\n",
    "                successful_companies += 1\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Pipeline completed: {successful_companies}/{len(companies)} companies processed successfully\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        if self.results:\n",
    "            self.results_df = pd.DataFrame(self.results)\n",
    "            return self.results_df\n",
    "        else:\n",
    "            print(\"No companies were processed successfully!\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def analyze_results(self):\n",
    "\n",
    "        if not hasattr(self, 'results_df') or self.results_df.empty:\n",
    "            print(\"No results to analyze!\")\n",
    "            return None\n",
    "\n",
    "        df = self.results_df\n",
    "        analysis = {}\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STOCK PREDICTION PIPELINE RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Model: {self.model_type} | Problem: {self.problem_type}\")\n",
    "        print(f\"Companies analyzed: {len(df)}\")\n",
    "        print(f\"Average samples per company: {df['n_samples'].mean():.0f}\")\n",
    "\n",
    "        # Overall performance metrics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"OVERALL PERFORMANCE\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        if self.problem_type == 'regression':\n",
    "            print(f\"Mean Squared Error:     {df['mse'].mean():.6f} (±{df['mse'].std():.6f})\")\n",
    "            print(f\"Mean Absolute Error:    {df['mae'].mean():.6f} (±{df['mae'].std():.6f})\")\n",
    "            print(f\"R² Score:              {df['r2'].mean():.4f} (±{df['r2'].std():.4f})\")\n",
    "\n",
    "        print(f\"Directional Accuracy:   {df['directional_accuracy'].mean():.4f} (±{df['directional_accuracy'].std():.4f})\")\n",
    "        print(f\"Matthews Correlation:   {df['mcc'].mean():.4f} (±{df['mcc'].std():.4f})\")\n",
    "        print(f\"F1 Score:              {df['f1'].mean():.4f} (±{df['f1'].std():.4f})\")\n",
    "        print(f\"Precision:             {df['precision'].mean():.4f} (±{df['precision'].std():.4f})\")\n",
    "        print(f\"Recall:                {df['recall'].mean():.4f} (±{df['recall'].std():.4f})\")\n",
    "\n",
    "        # Sector analysis\n",
    "        if 'sector' in df.columns and df['sector'].nunique() > 1:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"PERFORMANCE BY SECTOR\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "            sector_stats = df.groupby('sector').agg({\n",
    "                'directional_accuracy': ['mean', 'std', 'count'],\n",
    "                'mcc': ['mean', 'std'],\n",
    "                'r2': 'mean' if self.problem_type == 'regression' else lambda x: np.nan,\n",
    "                'mae': 'mean' if self.problem_type == 'regression' else lambda x: np.nan\n",
    "            }).round(4)\n",
    "\n",
    "            # Flatten column names\n",
    "            sector_stats.columns = ['_'.join(col).strip() if col[1] else col[0]\n",
    "                                  for col in sector_stats.columns]\n",
    "\n",
    "            # Sort by directional accuracy\n",
    "            sector_stats = sector_stats.sort_values('directional_accuracy_mean', ascending=False)\n",
    "\n",
    "            for sector, row in sector_stats.iterrows():\n",
    "                print(f\"{sector:<20} | Acc: {row['directional_accuracy_mean']:.3f}±{row['directional_accuracy_std']:.3f} | \"\n",
    "                      f\"MCC: {row['mcc_mean']:.3f} | Companies: {int(row['directional_accuracy_count'])}\")\n",
    "\n",
    "        # Top performers\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TOP 10 PERFORMERS (by Directional Accuracy)\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        top_performers = df.nlargest(10, 'directional_accuracy')\n",
    "        for _, row in top_performers.iterrows():\n",
    "            print(f\"{row['company']:<20} | {row['sector']:<15} | \"\n",
    "                  f\"Acc: {row['directional_accuracy']:.3f} | MCC: {row['mcc']:.3f}\")\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def save_results(self, output_path='stock_prediction_results.csv'):\n",
    "        \"\"\"Save results with timestamp and model info.\"\"\"\n",
    "        if hasattr(self, 'results_df') and not self.results_df.empty:\n",
    "            # Add metadata\n",
    "            timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{self.model_type}_{self.problem_type}_{timestamp}_results.csv\"\n",
    "            if output_path == 'stock_prediction_results.csv':\n",
    "                output_path = filename\n",
    "\n",
    "            self.results_df.to_csv(output_path, index=False)\n",
    "            print(f\"\\nResults saved to {output_path}\")\n",
    "            print(f\"Columns saved: {list(self.results_df.columns)}\")\n",
    "        else:\n",
    "            print(\"No results to save. Run the pipeline first!\")\n",
    "\n",
    "    def get_feature_importance_analysis(self):\n",
    "\n",
    "        print(\"Feature importance analysis not implemented yet.\")\n",
    "        print(\"Consider implementing SHAP values or permutation importance for better insights.\")\n",
    "        return None"
   ],
   "metadata": {
    "id": "fGvgtiPTiBQ4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlCKxsDxHHFe",
    "outputId": "36aa45f2-ea60-4af0-f4ea-d3804481245f"
   },
   "source": [
    "companies = pd.read_parquet('/content/stock_table.parquet')\n",
    "tweets = pd.read_parquet('/content/stock_tweets_withsentiment_withemotion_withstance_nomerge.parquet')\n",
    "stocks = pd.read_parquet('/content/stock_prices.parquet')\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "companies.columns = [x.lower() for x in companies.columns]\n",
    "tweets.columns = [x.lower() for x in tweets.columns]\n",
    "stocks.columns = [x.lower() for x in stocks.columns]\n",
    "\n",
    "tweets['stance_positive'] = (tweets['stance_label'] == 'Positive').astype(int)\n",
    "tweets['stance_negative'] = (tweets['stance_label'] == 'Negative').astype(int)\n",
    "\n",
    "tweets_merged = tweets.groupby(['date', 'ticker'], as_index=False).agg({\n",
    "    'text': lambda x: ' '.join(x),\n",
    "    'sentiment': lambda x: x.mean(),\n",
    "    'emotion_anger': 'sum',\n",
    "    'emotion_disgust': 'sum',\n",
    "    'emotion_fear': 'sum',\n",
    "    'emotion_joy': 'sum',\n",
    "    'emotion_neutral': 'sum',\n",
    "    'emotion_sadness': 'sum',\n",
    "    'emotion_surprize': 'sum',\n",
    "    'stance_positive': 'sum',\n",
    "    'stance_negative': 'sum'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tweets_merged['date'] = pd.to_datetime(tweets_merged['date'])\n",
    "stocks['date'] = pd.to_datetime(stocks['date'])\n",
    "\n",
    "\n",
    "\n",
    "master_df = pd.merge(\n",
    "    stocks,\n",
    "    tweets_merged,\n",
    "    on=[\"date\", \"ticker\"],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing tweet features with 0\n",
    "tweet_feature_cols = ['sentiment', 'emotion_anger', 'emotion_disgust', 'emotion_fear', 'emotion_joy', 'emotion_neutral', 'emotion_sadness', 'emotion_surprize', 'stance_positive', 'stance_negative']\n",
    "for col in tweet_feature_cols:\n",
    "    if col in master_df.columns:\n",
    "        master_df[col].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "master_df = pd.merge(master_df, companies[['ticker', 'sector', 'company']], on='ticker', how='left')\n",
    "\n",
    "\n",
    "feature_cols = ['open','high','low','volume']\n",
    "\n",
    "master_df = master_df.rename(columns={'close': 'close_price', 'company': 'company_name'})\n",
    "\n",
    "\n",
    "print(f\"Shape of master_df before dropping NaNs: {master_df.shape}\")\n",
    "print(f\"Shape of master_df after dropping NaNs: {master_df.shape}\")\n",
    "\n",
    "master_df.rename(columns={'close_price': 'close'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "master_df.sort_values(by=['ticker', 'date'], inplace=True)\n",
    "\n",
    "\n",
    "def apply_ta_indicators(df_group):\n",
    "    df_group.set_index(pd.DatetimeIndex(df_group['date']), inplace=True)\n",
    "    df_group.ta.ema(length=12, append=True)\n",
    "    df_group.ta.ema(length=26, append=True)\n",
    "    df_group.ta.ema(length=50, append=True)\n",
    "\n",
    "    df_group.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "\n",
    "\n",
    "    df_group.ta.rsi(length=14, append=True)\n",
    "    df_group.ta.stochrsi(length=14, append=True)\n",
    "\n",
    "    df_group.ta.atr(length=14, append=True)\n",
    "\n",
    "    bb = ta.bbands(df_group['close'], length=20, std=2)\n",
    "    df_group['BB_upper'] = bb['BBU_20_2.0']\n",
    "    df_group['BB_middle'] = bb['BBM_20_2.0']\n",
    "    df_group['BB_lower'] = bb['BBL_20_2.0']\n",
    "\n",
    "    df_group.ta.obv(append=True)\n",
    "    return df_group.reset_index(drop=True)\n",
    "\n",
    "master_df = master_df.groupby('ticker').apply(apply_ta_indicators)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "12_qUQQGGxKm",
    "outputId": "41eb4670-181a-465b-f565-290ccae2d22b"
   },
   "source": [
    "columns_to_check = ['EMA_12', 'EMA_26','EMA_50','MACD_12_26_9','MACDh_12_26_9','MACDs_12_26_9','RSI_14','ATRr_14','STOCHRSIk_14_14_3_3','STOCHRSId_14_14_3_3','ATRr_14','BB_upper','BB_middle','BB_lower','OBV']\n",
    "master_df = master_df.dropna(subset=columns_to_check)\n",
    "\n",
    "\n",
    "master_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(master_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(master_df.columns)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4D6EfvoYDrGY",
    "outputId": "a09ae550-5787-45c6-81aa-4bf74311f09f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "feature_columns = [\n",
    "    'open', 'high', 'low', 'close', 'volume',\n",
    "    'stance_positive', 'stance_negative'\n",
    "    'sentiment'\n",
    "]\n",
    "\n",
    "new_indicator_columns = [\n",
    "    'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9',\n",
    "    'RSI_14', 'ATRr_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3',\n",
    "    'BB_upper', 'BB_middle', 'BB_lower', 'OBV'\n",
    "]\n",
    "feature_columns.extend(new_indicator_columns)\n",
    "\n",
    "sequence_length=12\n",
    "\n",
    "\n",
    "\n",
    "all_pipelines = {}\n",
    "all_results_dfs = {}\n",
    "all_analyses = {}"
   ],
   "metadata": {
    "id": "ytoycwxUDtS3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "print(f\"\\n{'='*25}\\n  RUNNING PIPELINE FOR: LSTM\\n{'='*25}\\n\")\n",
    "\n",
    "pipeline_LSTM = StockPredictionPipeline(\n",
    "    df=master_df,\n",
    "    feature_columns=feature_columns,\n",
    "    model_type='BiLSTM',\n",
    "    sequence_length=sequence_length,\n",
    "    problem_type='regression'\n",
    ")\n",
    "\n",
    "\n",
    "results_LSTM = pipeline_LSTM.run_pipeline()\n",
    "\n",
    "\n",
    "if results_LSTM is not None and not results_LSTM.empty:\n",
    "    analysis_LSTM = pipeline_LSTM.analyze_results()\n",
    "    pipeline_LSTM.save_results(f'stock_prediction_results_LSTM.csv')\n",
    "\n",
    "    all_pipelines[\"LSTM\"] = pipeline_LSTM\n",
    "    all_results_dfs[\"LSTM\"] = results_LSTM\n",
    "    all_analyses[\"LSTM\"] = analysis_LSTM\n",
    "\n",
    "    print(\"\\nDisplaying first 5 rows of LSTM results:\")\n",
    "    display(results_LSTM.head())\n",
    "else:\n",
    "    print(f\"\\n[FAILED] Pipeline for LSTM did not produce any results.\")\n",
    "\n",
    "del pipeline_LSTM"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RBV6cWZNDuCW",
    "outputId": "07c614de-47f3-4115-a4f2-0a8e11405f69"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
